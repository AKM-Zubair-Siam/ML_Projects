{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1137ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "#sadness (0), \n",
    "#joy (1),\n",
    "#love (2),\n",
    "#anger (3),\n",
    "#fear (4),\n",
    "#surprise (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e67192d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953920e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2304bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i just feel really helpless and heavy hearted</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ive enjoyed being able to slouch about relax a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i gave up my internship with the dmrg and am f...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont know i feel so lost</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am a kindergarten teacher and i am thoroughl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0      i just feel really helpless and heavy hearted      4\n",
       "1  ive enjoyed being able to slouch about relax a...      0\n",
       "2  i gave up my internship with the dmrg and am f...      4\n",
       "3                         i dont know i feel so lost      0\n",
       "4  i am a kindergarten teacher and i am thoroughl...      4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb67642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93eac7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "315d81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating text and label columns\n",
    "X = df.drop(columns = 'label', axis = 1)\n",
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cf877d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text\n",
      "0           i just feel really helpless and heavy hearted\n",
      "1       ive enjoyed being able to slouch about relax a...\n",
      "2       i gave up my internship with the dmrg and am f...\n",
      "3                              i dont know i feel so lost\n",
      "4       i am a kindergarten teacher and i am thoroughl...\n",
      "...                                                   ...\n",
      "416804  i feel like telling these horny devils to find...\n",
      "416805  i began to realize that when i was feeling agi...\n",
      "416806  i feel very curious be why previous early dawn...\n",
      "416807  i feel that becuase of the tyranical nature of...\n",
      "416808  i think that after i had spent some time inves...\n",
      "\n",
      "[416809 rows x 1 columns] 0         4\n",
      "1         0\n",
      "2         4\n",
      "3         0\n",
      "4         4\n",
      "         ..\n",
      "416804    2\n",
      "416805    3\n",
      "416806    5\n",
      "416807    3\n",
      "416808    5\n",
      "Name: label, Length: 416809, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae5ab4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0a01b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]', ' ', text) #this regex is looking for words from a-z only. no numbers. commas and fullstops are replaced with a space as indicated by ' '\n",
    "    stemmed_content = stemmed_content.lower()#convert everything to lowercase letters\n",
    "    stemmed_content = stemmed_content.split()#convert everything in text to a list\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')] #reducing words to their root word - but for loop is removing all stop words\n",
    "    stemmed_content = ' '.join(stemmed_content)#joining all words\n",
    "    return stemmed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cf7430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1344ae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                          feel realli helpless heavi heart\n",
      "1         ive enjoy abl slouch relax unwind frankli need...\n",
      "2                      gave internship dmrg feel distraught\n",
      "3                                       dont know feel lost\n",
      "4         kindergarten teacher thoroughli weari job take...\n",
      "                                ...                        \n",
      "416804    feel like tell horni devil find site suit sort...\n",
      "416805    began realiz feel agit restless would thought ...\n",
      "416806      feel curiou previou earli dawn time seek troubl\n",
      "416807    feel becuas tyran natur govern el salvador sav...\n",
      "416808    think spent time investig surround thing start...\n",
      "Name: text, Length: 416809, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e70308bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Zooby\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3074685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the base word feel as every emotion has the word feel mentioned in them the highest number of times\n",
    "#also, the word feel itself is an emotion. It doesn't describe the \"type\" of emotion\n",
    "words_to_remove = ['feel']\n",
    "\n",
    "def remove_specific_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in words_to_remove]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "df['text'] = df['text'].apply(remove_specific_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bb5e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text'].values\n",
    "Y = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbdfa189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['realli helpless heavi heart'\n",
      " 'ive enjoy abl slouch relax unwind frankli need last week around end uni expo late start find bit listless never realli good thing'\n",
      " 'gave internship dmrg distraught' ...\n",
      " 'curiou previou earli dawn time seek troubl'\n",
      " 'becuas tyran natur govern el salvador savag social condit gener peopl set thought run shoe shop miguel marmol testimoney abl work'\n",
      " 'think spent time investig surround thing start curiou peopl'] [4 0 4 ... 5 3 5]\n"
     ]
    }
   ],
   "source": [
    "print(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8444273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting text data to numbers\n",
    "#main point of vectorizer is that it will create feature columns that it deems is important\n",
    "vectorizer = TfidfVectorizer() #basically finds words that are repeating the most to assign a value to it. similarly its inversely doing the opposite where if certain words are showing up and it doesnt have a value, it doesnt provide it value\n",
    "vectorizer.fit(X) #only fitting X bc Y already is all numbers (0,1)\n",
    "\n",
    "X = vectorizer.transform(X) #convert all values to respective features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddeaa173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 36847)\t0.3215265307671131\n",
      "  (0, 19829)\t0.496020837870654\n",
      "  (0, 19689)\t0.6544996046952785\n",
      "  (0, 19628)\t0.47139610293752254\n",
      "  (1, 49816)\t0.16298410435358318\n",
      "  (1, 48283)\t0.32139688881465395\n",
      "  (1, 47857)\t0.2852665843712161\n",
      "  (1, 45476)\t0.13136671138460346\n",
      "  (1, 42894)\t0.14231443865165985\n",
      "  (1, 41512)\t0.37416889523325847\n",
      "  (1, 37323)\t0.19203432174389404\n",
      "  (1, 36847)\t0.12018158556081865\n",
      "  (1, 30860)\t0.1558171286280182\n",
      "  (1, 30658)\t0.14152800961344186\n",
      "  (1, 26081)\t0.22301891253429965\n",
      "  (1, 25244)\t0.1762552351421575\n",
      "  (1, 25236)\t0.16521024271776186\n",
      "  (1, 22675)\t0.13722105023945944\n",
      "  (1, 18079)\t0.14988189770484192\n",
      "  (1, 16452)\t0.2645594323398258\n",
      "  (1, 15660)\t0.15987076149481164\n",
      "  (1, 14742)\t0.3700033697637148\n",
      "  (1, 13901)\t0.1809428807192119\n",
      "  (1, 13807)\t0.16594401737514589\n",
      "  (1, 4666)\t0.14027545104268804\n",
      "  :\t:\n",
      "  (416807, 40668)\t0.17460973208707112\n",
      "  (416807, 40641)\t0.18829996527590173\n",
      "  (416807, 40097)\t0.16215825437018758\n",
      "  (416807, 39205)\t0.2145058795903493\n",
      "  (416807, 38927)\t0.30407755646053936\n",
      "  (416807, 38596)\t0.1421800483332391\n",
      "  (416807, 33742)\t0.10305527184695154\n",
      "  (416807, 30490)\t0.16685811566404676\n",
      "  (416807, 28655)\t0.2995815603571753\n",
      "  (416807, 27573)\t0.3266730170988579\n",
      "  (416807, 18236)\t0.19995331221833\n",
      "  (416807, 17405)\t0.13674138361841776\n",
      "  (416807, 13444)\t0.2575429725198464\n",
      "  (416807, 9007)\t0.19708902535029052\n",
      "  (416807, 3941)\t0.24539864687381005\n",
      "  (416807, 127)\t0.13512934606186888\n",
      "  (416808, 45814)\t0.21525582194982876\n",
      "  (416808, 45480)\t0.22936133058561356\n",
      "  (416808, 45476)\t0.2351368992050415\n",
      "  (416808, 44018)\t0.4094181985388257\n",
      "  (416808, 42894)\t0.2547325381289815\n",
      "  (416808, 42388)\t0.3657435893467736\n",
      "  (416808, 33742)\t0.2333480533674276\n",
      "  (416808, 22357)\t0.5351235202662972\n",
      "  (416808, 10208)\t0.37239784290957545\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a29b500",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ba2ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = .2, stratify = Y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64cb53fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression(max_iter = 1000)\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'penalty': ['l1', 'l2'],              # Regularization type\n",
    "    'solver': ['liblinear']       # Optimization algorithm: note you can test with other solvers such as saga but run-time for saga algorithm is huge\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70b15f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(logistic_regression, param_grid, cv=5, scoring='accuracy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7968446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=1000),\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         'penalty': ['l1', 'l2'], 'solver': ['liblinear']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36a28729",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8af72331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set using the best model\n",
    "Y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a325a918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Accuracy on Test Set: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of the best model\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Accuracy on Test Set: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76e47d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "joy\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the model on test data\n",
    "X_new = X_test[400]\n",
    "\n",
    "prediction = best_model.predict(X_new)\n",
    "print(prediction)\n",
    "\n",
    "if (prediction[0]==0):\n",
    "    print('sadness')\n",
    "elif (prediction[0]==1):\n",
    "    print('joy')\n",
    "elif (prediction[0]==2):\n",
    "    print('love')\n",
    "elif (prediction[0]==3):\n",
    "    print('anger')\n",
    "elif (prediction[0]==4):\n",
    "    print('fear')\n",
    "elif (prediction[0]==5):\n",
    "    print('surprise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29133e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print (Y_test[400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5163cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba82ffd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10554bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9152fa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
